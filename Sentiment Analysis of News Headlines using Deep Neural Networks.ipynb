{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a5447a3",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of News Headlines using Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdbfdf2",
   "metadata": {},
   "source": [
    "### Installing necessary packages, importing libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a083411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10061]\n",
      "[nltk_data]     No connection could be made because the target machine\n",
      "[nltk_data]     actively refused it>\n"
     ]
    }
   ],
   "source": [
    "#install needed packages\n",
    "!pip install snorkel\n",
    "!pip install spacy\n",
    "!pip install textblob\n",
    "!pip install tensorflow\n",
    "\n",
    "#import libraries and modules\n",
    "import io\n",
    "import pandas as pd\n",
    "#Snorkel\n",
    "from snorkel.labeling import LabelingFunction\n",
    "import re\n",
    "from snorkel.preprocess import preprocessor\n",
    "from textblob import TextBlob\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "from snorkel.labeling import labeling_function\n",
    "#NLP packages\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import nltk.tokenize\n",
    "punc = string.punctuation\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "##Deep learning libraries and APIs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f9d1e1",
   "metadata": {},
   "source": [
    "### Getting News Titles and Article from BBC API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9ffc8f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Winter storm's icy blast hits 200 million in US\n",
      "2 US winter storm: Stranded Native Americans burn clothes for warmth\n",
      "3 Iran protests: Activist Narges Mohammadi details 'abuse' of detained women\n",
      "4 Russia-Ukraine war: Strikes on Kherson kill seven\n",
      "5 South Africa: Fuel tanker explosion kills several near hospital\n",
      "6 Scuba diving Santa makes waves in Florida\n",
      "7 Megan Thee Stallion: US jury finds Tory Lanez guilty of shooting hip-hop star\n",
      "8 Bethlehem sees Christmas tourism boost after two-year Covid hiatus\n",
      "9 Afghanistan protests: Taliban use water cannon on women opposing university ban\n",
      "10 The Sun apologises over Jeremy Clarkson's Meghan column\n"
     ]
    }
   ],
   "source": [
    "# importing requests package\n",
    "import requests    \n",
    " \n",
    "def NewsFromBBC():\n",
    "     \n",
    "    # BBC news api\n",
    "    # following query parameters are used\n",
    "    # source, sortBy and apiKey\n",
    "    query_params = {\n",
    "      \"source\": \"bbc-news\",\n",
    "      \"sortBy\": \"top\",\n",
    "      \"apiKey\": \"4dbc17e007ab436fb66416009dfb59a8\"\n",
    "    }\n",
    "    main_url = \" https://newsapi.org/v1/articles\"\n",
    " \n",
    "    # fetching data in json format\n",
    "    res = requests.get(main_url, params=query_params)\n",
    "    open_bbc_page = res.json()\n",
    " \n",
    "    # getting all articles in a string article\n",
    "    article = open_bbc_page[\"articles\"]\n",
    "    # empty list which will contain all trending news\n",
    "    results = []\n",
    "     \n",
    "    for ar in article:\n",
    "        results.append(ar[\"title\"])\n",
    "         \n",
    "    for i in range(len(results)):\n",
    "         \n",
    "        # printing all trending news\n",
    "        print(i + 1, results[i])\n",
    "    \n",
    "    return results, article\n",
    "                  \n",
    " \n",
    "# Driver Code\n",
    "if __name__ == '__main__':\n",
    "     \n",
    "    # function call.\n",
    "    title, article = NewsFromBBC()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c964438",
   "metadata": {},
   "source": [
    "### Loading local data set for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80cd1b7",
   "metadata": {},
   "source": [
    "#### The dataset which is used in this project is called the ‘ Million News Headlines’ dataset and it is available on Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a23996a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uplaod the data from your local directory\n",
    "# store the dataset as a Pandas Dataframe\n",
    "\n",
    "file=('abcnews-date1-text.csv')\n",
    "df = pd.read_csv(file)\n",
    "#conduct some data cleaning\n",
    "df = df.drop(['publish_date'], axis=1)\n",
    "df = df.rename(columns = {'headline_text': 'text'})\n",
    "df['text'] = df['text'].astype(str)\n",
    "#check the data info\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c28d7",
   "metadata": {},
   "source": [
    "Shape of the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c7759cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6284, 2)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6bc67a",
   "metadata": {},
   "source": [
    "### Snorkel Labeling Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a5e87",
   "metadata": {},
   "source": [
    "Because the dataset is unlabelled, we will employ Snorkel, to come up with heuristics and programmatic rules using functions which assign labels of two classes that differentiate if the headline is positive (1) or negative (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e7f1c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define constants to represent the class labels :positive, negative, and abstain\n",
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "#define function which looks into the input words to represent a proper label\n",
    "def keyword_lookup(x, keywords, label):  \n",
    "    if any(word in x.text.lower() for word in keywords):\n",
    "        return label\n",
    "    return ABSTAIN\n",
    "#define function which assigns a correct label\n",
    "def make_keyword_lf(keywords, label=POSITIVE):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords[0]}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label))\n",
    "\n",
    "\"\"\"positive news might contain the following words' \"\"\"\n",
    "keyword_positive = make_keyword_lf(keywords=['boosts','tribute','legacy','christmas','faith','opportunity','undaunted', 'great', 'develops', 'promising', 'ambitious', 'delighted', 'record', 'win', 'breakthrough', 'recover', 'achievement', 'peace', 'party', 'hope', 'unhappy','flourish', 'respect', 'partnership', 'champion', 'positive', 'happy', 'bright', 'confident', 'encouraged', 'perfect', 'complete', 'assured' ])\n",
    "\"\"\"negative news might contain the following words\"\"\"\n",
    "keyword_negative = make_keyword_lf(keywords=['war','blast','dead','illegal','blast','burn','guilty','dead','storm','solidiers','fire', 'turmoil', 'injured','trouble', 'aggressive', 'killed', 'coup', 'evasion', 'strike', 'troops', 'dismisses', 'attacks', 'defeat', 'damage', 'dishonest', 'dead', 'fear', 'foul', 'fails', 'hostile', 'cuts', 'accusations', 'victims',  'death', 'unrest', 'fraud', 'dispute', 'destruction', 'battle', 'unhappy', 'bad', 'alarming', 'angry', 'anxious', 'dirty', 'pain', 'poison', 'unfair', 'unhealthy'\n",
    "                                              ], label=NEGATIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3fe3f4",
   "metadata": {},
   "source": [
    "### Textblob Labeling Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86154359",
   "metadata": {},
   "source": [
    "Another set of labelling functions were implemented through TextBlob tool, a pretrained sentiment analyzer. We will create a Pre-processor that runs TextBlob on our headlines, then extracts the polarity and subjectivity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e24a1a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up a preprocessor function to determine polarity & subjectivity using textlob pretrained classifier \n",
    "@preprocessor(memoize=True)\n",
    "def textblob_sentiment(x):\n",
    "    scores = TextBlob(x.text)\n",
    "    x.polarity = scores.sentiment.polarity\n",
    "    x.subjectivity = scores.sentiment.subjectivity\n",
    "    return x\n",
    "#find polarity\n",
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_polarity(x):\n",
    "    return POSITIVE if x.polarity > 0.6 else ABSTAIN\n",
    "#find subjectivity \n",
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_subjectivity(x):\n",
    "    return POSITIVE if x.subjectivity >= 0.5 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948bbe91",
   "metadata": {},
   "source": [
    " The next step is to **combine all the labelling functions** and apply it on our dataset. Then, we fit the label_model to predict and generate the positive and negative classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6d61fb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6284/6284 [00:11<00:00, 551.19it/s]\n",
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "  0%|          | 0/100 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=0.091]\n",
      "INFO:root:[10 epochs]: TRAIN:[loss=0.027]\n",
      "INFO:root:[20 epochs]: TRAIN:[loss=0.018]\n",
      "INFO:root:[30 epochs]: TRAIN:[loss=0.015]\n",
      "INFO:root:[40 epochs]: TRAIN:[loss=0.012]\n",
      "INFO:root:[50 epochs]: TRAIN:[loss=0.012]\n",
      "INFO:root:[60 epochs]: TRAIN:[loss=0.011]\n",
      " 61%|██████    | 61/100 [00:00<00:00, 608.07epoch/s]INFO:root:[70 epochs]: TRAIN:[loss=0.011]\n",
      "INFO:root:[80 epochs]: TRAIN:[loss=0.010]\n",
      "INFO:root:[90 epochs]: TRAIN:[loss=0.010]\n",
      "100%|██████████| 100/100 [00:00<00:00, 628.72epoch/s]\n",
      "INFO:root:Finished Training\n"
     ]
    }
   ],
   "source": [
    "#combine all the labeling functions \n",
    "lfs = [keyword_positive, keyword_negative, textblob_polarity, textblob_subjectivity ]\n",
    "#apply the lfs on the dataframe\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_snorkel = applier.apply(df=df)\n",
    "#apply the label model\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "#fit on the data\n",
    "label_model.fit(L_snorkel)\n",
    "#predict and create the labels\n",
    "df[\"label\"] = label_model.predict(L=L_snorkel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a3551bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3387\n",
       "0    2897\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering out unlabeled data points\n",
    "df= df.loc[df.label.isin([0,1]), :]\n",
    "#find the label counts \n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c11b918",
   "metadata": {},
   "source": [
    "We can notice that after dropping the unlabelled data points (as shown above), we have around 3387 positive labels and 2897 negative which will be sufficient to build our sentiment classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab3413",
   "metadata": {},
   "source": [
    "## Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "80a1b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##store headlines and labels in respective lists\n",
    "text = list(df['text'])\n",
    "labels = list(df['label'])\n",
    "##sentences\n",
    "training_text = text[0:10000]\n",
    "testing_text = text[10000:]\n",
    "##labels\n",
    "training_labels = labels[0:10000]\n",
    "testing_labels = labels[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760aad9",
   "metadata": {},
   "source": [
    "### Set up the tokenizer from Tensor to pre-process the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c1d32d",
   "metadata": {},
   "source": [
    "In this cell, we use the **word tokenizer from tensorflow.keras** to create word encodings (dictionary with key-value pairs) and sequences using **texs_to_sequences instance**, and then we pad these sequences to make it of equal length using the **pad_sequences instance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fe18c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess \n",
    "tokenizer = Tokenizer(num_words=10000, oov_token= \"<OOV>\")\n",
    "tokenizer.fit_on_texts(training_text)\n",
    "word_index = tokenizer.word_index\n",
    "training_sequences = tokenizer.texts_to_sequences(training_text)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=120, padding='post', truncating='post')\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_text)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=120, padding='post', truncating='post')\n",
    "# convert lists into numpy arrays to make it work with TensorFlow \n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc85f35",
   "metadata": {},
   "source": [
    "In this step, we use the **word tokenizer from tensorflow.keras** to create word encodings (dictionary with key-value pairs) and sequences using **texs_to_sequences instance**, and then we pad these sequences to make it of equal length using the **pad_sequences instance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7100005e",
   "metadata": {},
   "source": [
    "### Define & train the Sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48144f1",
   "metadata": {},
   "source": [
    "We build the model with an **embedding layer** of a vocab size, embedding dimension, and input length. We also add a **dense layer RelU** which asks the model to classify the instances into two classes as positive or negative and other final **sigmoid layer** which outputs probabilities between 0 or 1. We can simply play with the hyperparameters within each layer to increase model performance. Then, we compile the model with an optimizer and metric performance and we train it on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6aa20bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 120, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d_5   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 24)                408       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,433\n",
      "Trainable params: 160,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 16, input_length=120),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "##compile the model\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "eeb8acad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "197/197 - 2s - loss: 0.6898 - accuracy: 0.5390 - 2s/epoch - 12ms/step\n",
      "Epoch 2/15\n",
      "197/197 - 1s - loss: 0.6832 - accuracy: 0.5404 - 722ms/epoch - 4ms/step\n",
      "Epoch 3/15\n",
      "197/197 - 1s - loss: 0.6378 - accuracy: 0.6809 - 715ms/epoch - 4ms/step\n",
      "Epoch 4/15\n",
      "197/197 - 1s - loss: 0.5031 - accuracy: 0.8488 - 718ms/epoch - 4ms/step\n",
      "Epoch 5/15\n",
      "197/197 - 1s - loss: 0.3472 - accuracy: 0.9182 - 737ms/epoch - 4ms/step\n",
      "Epoch 6/15\n",
      "197/197 - 1s - loss: 0.2386 - accuracy: 0.9476 - 755ms/epoch - 4ms/step\n",
      "Epoch 7/15\n",
      "197/197 - 1s - loss: 0.1725 - accuracy: 0.9631 - 746ms/epoch - 4ms/step\n",
      "Epoch 8/15\n",
      "197/197 - 1s - loss: 0.1257 - accuracy: 0.9760 - 748ms/epoch - 4ms/step\n",
      "Epoch 9/15\n",
      "197/197 - 1s - loss: 0.0970 - accuracy: 0.9820 - 755ms/epoch - 4ms/step\n",
      "Epoch 10/15\n",
      "197/197 - 1s - loss: 0.0756 - accuracy: 0.9860 - 721ms/epoch - 4ms/step\n",
      "Epoch 11/15\n",
      "197/197 - 1s - loss: 0.0607 - accuracy: 0.9893 - 719ms/epoch - 4ms/step\n",
      "Epoch 12/15\n",
      "197/197 - 1s - loss: 0.0493 - accuracy: 0.9919 - 732ms/epoch - 4ms/step\n",
      "Epoch 13/15\n",
      "197/197 - 1s - loss: 0.0412 - accuracy: 0.9940 - 716ms/epoch - 4ms/step\n",
      "Epoch 14/15\n",
      "197/197 - 1s - loss: 0.0343 - accuracy: 0.9946 - 721ms/epoch - 4ms/step\n",
      "Epoch 15/15\n",
      "197/197 - 1s - loss: 0.0278 - accuracy: 0.9957 - 720ms/epoch - 4ms/step\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "history = model.fit(training_padded, \n",
    "                    training_labels, \n",
    "                    epochs=num_epochs, \n",
    "                    validation_data=(testing_padded, testing_labels), \n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cce4f",
   "metadata": {},
   "source": [
    "We can further check that **our built neural network model** with **15 running epochs** has a **very good accuracy of 99.57%** , decreasing validation loss and increasing validation accuracy **which assure a powerful predictive performance** and a low risk of (generalisation) overfitting error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ec98c",
   "metadata": {},
   "source": [
    "## Using the model to predict on the News Headline taken from the BBC API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9f1751f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "[\"Winter storm's icy blast hits 200 million in US\"] :  Negative\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "['US winter storm: Stranded Native Americans burn clothes for warmth'] :  Negative\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "['Megan Thee Stallion: US jury finds Tory Lanez guilty of shooting hip-hop star'] :  Negative\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "['Wind, snow and floods combine for historic US storm'] :  Negative\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "['Winter storm strands Canadian family in RV in Texas'] :  Negative\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "['Boiling water turns to snow in frigid Montana'] :  Positive\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[\"King's Christmas message to pay tribute to Queen's legacy\"] :  Positive\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "['At least 20 dead in Russia illegal care home fire'] :  Negative\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[\"The Sun apologises over Jeremy Clarkson's Meghan column\"] :  Positive\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "['Paris shooting: Two dead and several injured in attack'] :  Negative\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    new_headline = [title[i]]\n",
    "    #print(new_headline)\n",
    "    #prepare the sequences of the sentences in question\n",
    "    sequences = tokenizer.texts_to_sequences(new_headline)\n",
    "    padded_seqs = pad_sequences(sequences, maxlen=120, padding='post', truncating='post')\n",
    "    #print(model.predict(padded_seqs))\n",
    "    sent_value = model.predict(padded_seqs)\n",
    "    if sent_value > 0.5:\n",
    "     sentiment=\"Positive\"\n",
    "    else:\n",
    "     sentiment=\"Negative\"\n",
    "    print(new_headline,str(\": \"), sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d569c7",
   "metadata": {},
   "source": [
    "# You can see that our model is working very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2c4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
